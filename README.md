
# ğŸ“„ RAG PDF Chat (Local)

A **local Retrieval-Augmented Generation (RAG) system** that allows users to **ask natural language questions from PDF documents** using a **terminal-based chat interface**.

The system runs **fully offline** using **Ollama**, **FAISS**, and **LangChain**, and supports **conversation memory**, **source citation**, and a **Streamlit-like terminal UI** built with **Textual**.

---

## ğŸš€ Features

* ğŸ“š **PDF Question Answering** using RAG
* ğŸ§  **Semantic Search** with FAISS vector store
* ğŸ¤– **Local LLMs via Ollama** (phi3.5 / llama3.1)
* ğŸ’¬ **Chat-based Interface** (Terminal UI)
* ğŸ§¾ **Source Citation Toggle** (view retrieved chunks)
* ğŸ§  **Conversation Memory** (context-aware follow-ups)
* ğŸ”’ **Fully Offline & Private**
* âš¡ **FAISS persistence** (no re-embedding every run)

---

## ğŸ› ï¸ Tech Stack

* **Python 3.11**
* **LangChain**
* **FAISS**
* **Ollama**
* **HuggingFace Sentence Transformers**
* **Textual (Terminal UI)**

---

## ğŸ“‚ Project Structure

```text
rag-pdf-chat/
â”‚
â”œâ”€â”€ app.py                  # Entry point
â”œâ”€â”€ README.md
â”œâ”€â”€ .gitignore
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ sample.pdf          # Input PDF(s)
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ loader.py           # PDF loading
â”‚   â”œâ”€â”€ chunking.py         # Text chunking
â”‚   â”œâ”€â”€ embeddings.py       # Embedding model
â”‚   â”œâ”€â”€ vectorstore.py      # FAISS store (load/save)
â”‚   â”œâ”€â”€ rag_chain.py        # RAG pipeline
â”‚   â”œâ”€â”€ tui_app.py          # Terminal UI (Textual)
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ faiss_index/             # Saved vector index (ignored in git)
â””â”€â”€ venv/                    # Virtual environment (ignored)
```

---

## âš™ï¸ Setup Instructions

### 1ï¸âƒ£ Clone the repository

```bash
git clone https://github.com/your-username/rag-pdf-chat.git
cd rag-pdf-chat
```

---

### 2ï¸âƒ£ Create & activate virtual environment

```bash
python -m venv venv
source venv/bin/activate
```

---

### 3ï¸âƒ£ Install dependencies

```bash
pip install -r requirements.txt
```

*(Or install manually if you prefer)*

---

### 4ï¸âƒ£ Install & run Ollama

Install Ollama from:
ğŸ‘‰ [https://ollama.com](https://ollama.com)

Pull a model (example):

```bash
ollama pull phi3.5
```

---

## â–¶ï¸ Run the Application

```bash
python app.py
```

You will see a **full-screen terminal UI**.

---

## ğŸ’¬ How to Use

* Type your question and press **Enter**
* Ask follow-up questions (memory enabled)
* Press **`s`** â†’ toggle source citations
* Type **`:clear`** â†’ clear conversation memory
* Type **`exit`** â†’ quit the app

---

## ğŸ§  Example

```text
You: what is collection in java?
Assistant: A Collection in Java is part of the Java Collections Framework...

You: explain more
Assistant: Collections provide standardized data structures such as List, Set, and Map...
```

---

## ğŸ” How It Works (Architecture)

1. PDF is loaded and split into chunks
2. Chunks are embedded using a sentence transformer
3. FAISS stores vectors for semantic search
4. User query retrieves relevant chunks
5. Retrieved context + chat history are passed to the LLM
6. LLM generates a grounded response

---

## ğŸ¯ Why This Project Matters

* Demonstrates **real-world RAG implementation**
* Uses **modern LangChain Runnable architecture**
* Shows **offline-first AI design**
* Includes **custom terminal UI**
* Suitable for **portfolio, interviews, and demos**

---

## ğŸ“Œ Future Improvements

* Multiple PDF selection
* Live LLM model switching
* Streaming token output
* Export chat history
* Web UI version

---

## ğŸ‘¤ Author

**Pratik**  
Computer Science Student | Web Development | AI | Cybersecurity

---

## â­ Acknowledgements

* LangChain
* HuggingFace
* Ollama
* FAISS
* Textual

---
